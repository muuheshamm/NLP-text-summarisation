{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jsonlines\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import ISRIStemmer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def read_jsonl_to_dataframe(file_path):\n",
    "    data = []\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        for line in reader:\n",
    "            data.append(line)\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "train_df = read_jsonl_to_dataframe('/content/arabic_train.jsonl')\n",
    "val_df = read_jsonl_to_dataframe('/content/arabic_val.jsonl')\n",
    "test_df = read_jsonl_to_dataframe('/content/arabic_test.jsonl')\n",
    "\n",
    "train_df = train_df.drop(['id', 'url', 'title'], axis=1)\n",
    "test_df = test_df.drop(['id', 'url', 'title'], axis=1)\n",
    "val_df = val_df.drop(['id', 'url', 'title'], axis=1)\n",
    "\n",
    "stop_words = set(stopwords.words('arabic'))\n",
    "stemmer = ISRIStemmer()\n",
    "\n",
    "def clean_text_arabic(text):\n",
    "    text = re.sub(r\"[^\\u0600-\\u06FF\\u0750-\\u077F\\u08A0-\\u08FF\\uFB50-\\uFBC2\\uFBD3-\\uFDFF\\uFE70-\\uFEFE]\", \" \", text)\n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    cleaned_text = ' '.join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_text_data(df):\n",
    "    df['summary'] = df['summary'].apply(clean_text_arabic)\n",
    "    df['text'] = df['text'].apply(clean_text_arabic)\n",
    "    return df\n",
    "\n",
    "train_df = clean_text_data(train_df)\n",
    "test_df = clean_text_data(test_df)\n",
    "val_df = clean_text_data(val_df)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/AraT5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"UBC-NLP/AraT5-base\")\n",
    "\n",
    "def generate_summary(text):\n",
    "    max_chunk_len = 512  # Maximum length per chunk\n",
    "    text_chunks = [text[i:i+max_chunk_len] for i in range(0, len(text), max_chunk_len)]\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in text_chunks:\n",
    "        inputs = tokenizer.encode_plus(chunk, padding=\"max_length\", truncation=True, max_length=max_chunk_len, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        summary_ids = model.generate(input_ids, attention_mask=attention_mask, num_beams=4, max_length=100, early_stopping=True)\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return \" \".join(summaries)\n",
    "\n",
    "test_df[\"predicted_summary\"] = test_df[\"text\"].apply(generate_summary)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "scores = [scorer.score(test_df[\"summary\"].iloc[i], test_df[\"predicted_summary\"].iloc[i]) for i in range(test_df.shape[0])]\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
