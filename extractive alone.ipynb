{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tZw0Auh-e0Xc"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "def read_jsonl_to_dataframe(file_path):\n",
        "    data = []\n",
        "    with jsonlines.open(file_path) as reader:\n",
        "        for line in reader:\n",
        "            data.append(line)\n",
        "    df = pd.DataFrame(data)\n",
        "    return df"
      ],
      "metadata": {
        "id": "J9pf28bTftdX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R6dMimfet0-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = read_jsonl_to_dataframe('/content/validation_data.jsonl')"
      ],
      "metadata": {
        "id": "uE23n96GgMcK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_arabic_text(text, summary_length=3):\n",
        "    # Step 1: Tokenize the text into sentences\n",
        "    nltk.download('punkt')\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Step 2: Calculate sentence similarity using TF-IDF\n",
        "    vectorizer = CountVectorizer()\n",
        "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
        "    tfidf_transformer = TfidfTransformer()\n",
        "    tfidf_matrix = tfidf_transformer.fit_transform(sentence_vectors)\n",
        "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    # Step 3: Select the most important sentences based on similarity\n",
        "    sentence_scores = [(i, sum(similarity_matrix[i])) for i in range(len(sentences))]\n",
        "    sentence_scores = sorted(sentence_scores, key=lambda x: x[1], reverse=True)\n",
        "    selected_sentences = [sentences[i] for (i, score) in sentence_scores[:summary_length]]\n",
        "\n",
        "    # Step 4: Combine the selected sentences to create the summary\n",
        "    summary = ' '.join(selected_sentences)\n",
        "\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "-qJVpEe5gaBS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open('summaries2.jsonl', mode='w') as writer:\n",
        "\n",
        "    # Iterate through each row in the dataset and generate summaries\n",
        "    for index, row in data.iterrows():\n",
        "        example_id = row['example_id']\n",
        "        arabic_text = row['paragraph']\n",
        "        summary = summarize_arabic_text(arabic_text, summary_length=1)\n",
        "\n",
        "        # Create a dictionary for the summary\n",
        "        summary_dict = {\n",
        "            'example_id': example_id,\n",
        "            'summary': summary\n",
        "        }\n",
        "\n",
        "        # Append the summary dictionary to the list\n",
        "        writer.write(summary_dict)\n",
        "\n",
        "        print(f\"Summary for Example ID {example_id}: {summary}\")\n",
        ""
      ],
      "metadata": {
        "id": "FrHnAq_IgtCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qKsoS1dlt1yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2lOlWf6lt11Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jq2KU8s2t134"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "plP1M8Wht16Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QPZOtM9Kt18t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}